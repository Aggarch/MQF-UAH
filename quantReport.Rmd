---
title: "Código del Modelo"
author: "Analítica"
date: "23/03/2020"
output: word_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libreries, include=FALSE}

library(tidyverse)
library(lubridate)
library(pool)
library(formattable)
library(kableExtra)
library(janitor)
library(knitr)
library(gridExtra)
library(tidyquant)
library(xml2)
library(tidyquant)
library(timetk)
library(corrplot)
library(alphavantager)
library(plotly)
library(prophet)

```

## R Markdown

Este documento fue generado a través de R Markdown un paquete de formatos que nos permite compartir información de sintaxis, 
utilizando HTML, PDF y en este caso documentos de Microsoft Word, el propósito de este capítulo, es compartir nuestra investigación sobre el código del modelo y cuáles son las principales funciones que hemos utilizado y su funcionamiento, 
junto con ejemplos prácticos para la interpretación del usuario. 
Si desea saber más sobre el paquete en cuestón visite <http://rmarkdown.rstudio.com>.

En caso de que este observando este código directamente desde la carpeta de contenidos de la tesis, y no en el documento final, tenga en cuenta que al presionar el boton  **Knit** en el borde superior del IDE Rstudio podrá generar de manera automatica el documento en Word.

El código del modelo, será presentado en cuadros de outputs, que exhibiran el resultado final, así como comentarios y referencias sobre la fuente de los datos y su significado e importancia para el modelo. 

Tener en cuenta que el modelo, este documento y su respectivo instrumento ó aplicativo, utilizaran los siguientes paquetes, de los cuáles podrá acceder a su documentación oficial con los links disponibles en la última página de este documento:


```{r librerias, eval=F}

library(tidyverse)      # Integración de Recursos para Data Science. 
library(lubridate)      # Configuración de strings de fechas. 
library(kableExtra)     # Creación de Tablas tipo HTML.
library(knitr)          # Generación de Informes dínamicos.
library(tidyquant)      # Integración para analísis Financiero.
library(xml2)           # Conversión de fragmentos a HTML
library(timetk)         # Herramienta para trabajar Series de Tiempo.
library(corrplot)       # Librería para dibujar matrices de correlación.
library(alphavantager)  # API para importar datos de Mercado.
library(plotly)         # Herramienta de Visualización interactiva.
library(prophet)        # Analísis de series temporales. 

```

Como puede apreciarse en la documentación, cada paquete contiene pluralidad de funciones, por lo cual la lectura de la documentación es clave
para aquellos que deseen un mejor entendimiento de la dinámica expuesta. 


## Logíca General

Para explorar el código seguiremos la logica del modelo general, para analítica, expuesto previamente en el capítulo sobre el paradigma
del Data Science y su importancia para los procesos de toma de decisiones. 

Este modelo general ha sido extraido del libro R for Data Science, un esfuerzo colaborativo, disponible online :  
acceda al  libro a través de este link: <https://r4ds.had.co.nz/explore-intro.html>.

```{r ds_flow, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("data_science.png")
```

Es importante tener en cuenta que el conjunto de heramientas, armoniza en su filosofia Tidy, que esencialmente significa, que los conjuntos de datos, tienen una estructura basica, donde cada columna representa una variable y cada fila una observación, no todos los outputs de las funciones utilizadas, tiene la estructura deseada, pero gracias a las herramientas seleccionadas, su manipulación puede ser automatizada, con el fin de que cumplan con la arquitectura base. 
para explorar más sobre esta "filosofia" del Data Science invitamos a la siguiente lectura del Journal of Statistical Software: <"https://vita.had.co.nz/papers/tidy-data.pdf">




## Importación de Datos:

Inicialmente se importaran los datos, estos serán importados directamente de la Reserva Fededral de St. Louis,
los datos pueden explorarse en la siguiente página web: <https://fred.stlouisfed.org/>
donde cada activo o indicador economico, tiene un ticket que puede se encontrado buscando el nombre general. 
Sín embargo para automatizar la descarga necesitamos conocer el nombre tecnico e introducirlo como argumento de la función de importación
que utiliaremos, funcion **("tq_get")** dicha funcion posee tres argumentos principales, la variable **(x)** que se refiere al activo ó indicador a analizar, para que sea interpretable por la función, usar el ticket respectivo, el argumento "get" hace referencia al api o lugar desde el cual,
se pueden importar los datos, en este caso seleccionamos la opcion **"economic.data"** puesto que nos conecta con el API de la FRED y finalmente un rango de fechas donde: **"from"** es el momento anterior y **"to"** el posterior.
Se ha seleccionado este paquete por su flexibilidad y por la calidad, prestigio e importancia de la institucion que los comparte

### Ejercicio de Importación:

Importar la medición sobre los niveles de incertidumbre...

```{r import}
EPU <- tq_get("USEPUINDXD", get = "economic.data", from = "2017-01-01")

EPU <- EPU %>% arrange(desc(date))

EPU
# Aplicación sencilla de la función.

```

El ejemplo presentado, arroja como output un tibble o data frame, que contiene el símbolo importado, la fecha y su precio en el momento observado, se debe tener en cuenta que en este caso especifico, no se trata de un precio sino de un indicador de incertidumbre económica, 
El indicador en cuestión, es conocido como **"EPU"** cuyas siglas corresponden a (ECONOMIC POLICY UNCERTAINTY), este indicador fue creado por el **NBER** (NATIONAL BUREAU OF ECONOMIC RESEARCH), el indicador fue construido para medir los niveles de incertidumbre de USA a través de proxies que identifican y calculan la frecuencia con la que se hace mención a la incertidumbre respecto a las politicas públicas económicas de impacto global, en los principales periodicos del país utilizando datos de primera calidad, para explorar más sobre este indicador específico, los remitimos a la documentación oficial de la institución creadora, 
y a la serie temporal disponible en **FRED.** 

**NBER**, Measuring Economic Policy Uncertainty : <http://www.policyuncertainty.com/media/BakerBloomDavis.pdf>

**EPU** , FRED time series for Economic Policy Uncertainty: <https://fred.stlouisfed.org/series/USEPUINDXD/>

Se debe tener en cuenta que la variable "EPU" gracias al asignador del código, ya contiene los datos importados, es decir que para realizar cualquier tipo de modificación ó transformación sobre los datos importados, basta con llamar la variable u objeto R y anexar los cambios ó transformaciones deseadas, es por esta razón, en el ejemplo planteado, después de asignar la variable "EPU" se anexo un ordenamiento que nos permite observar los datos más recientes en el tiempo, donde el operador **("%>%")** traduce "AND THEN" en este caso seguido de **"arrange(desc(date))"**, sintaxis que logra ordenar la data de forma tal que las fechas se ordenen de mayor a menor para lograr observar los puntos más recientes en el tiempo. 


## Visualización:

Para visualizar utilizaremos el paquete plotly, este paquete permite al usuario interactuar con el objeto representado, siempre y cuando este en un ambiente que permita dicha interaccion de lo contrario, solo cumplirá el proposito visual, para este ejemplo, visualizaremos la variable importada, **EPU** utilizando el paquete **ggplot2**, esta herramienta nos permite explicar a la funcion con facilidad, cuales con las variables a tener en cuenta para graficar en el plano cartesiano, en nuestro caso, el eje horizontal sera el tiempo y el verticar, el nivel del indicador.

Ademas de graficar la evolucion en el tiempo de la variable **EPU**, se trazará una regresion lineal simple **(orange)**. 

### **EPU**

(Economic Policy Uncertainty, codigo para graficar ) :

```{r visual_code}
# A ggplot of the Global Economic Policy Uncertainty : 
uncertainty_index <- 
#  ggplotly(
    EPU %>%
      filter(date >= "2019-10-20") %>% 
      ggplot(aes(x = date, y = price))+
      theme_tq()+
      geom_point(color = "steelblue", size = 2)+
      geom_line(color = "gray", size = 1)+
      geom_smooth(color = "orange", method = loess)+
      labs(x = "Date", y = "Economic Policy Uncertainty Index",
           title = "Global Economic Policy Uncertainty Evolution")  
#    )

# Se puede apreciar de forma muy intuitiva, a que corresponde cada variable y color de la funcion ggplot,
# ggplot no necesita ser importado puesto que es contenido por tidyverse. 

```

### **EPU**

(Economic Policy Uncertainty, Representación Visual) :


```{r visual, warning=FALSE}
uncertainty_index

# Se observa un incremento considerable en los niveles de incertidumbre, producto de la coyuntuta actual, que a la fecha de ejecucion, de este reporte dinamico, es el COVID-19

Sys.Date()

```

## Correlación

Hasta ahora, un codigo tan simple ya es capaz de añadir valor a nuestro analísis, las razones por las que añade valor son varias, pero conviene destacar la calidad de la fuente primaria, la automatización del proceso y la calidad de la grafica, es de tener en cuenta que este documento en Word fue credo directamente desde Rstudio con relativamente poco esfuerzo, Word es un ambiente no reactivo, es decir que no cambia de manera dinamica, sin embargo el reporte cada vez que se ejecuta en el **IDE** de **R** intera en cada uno de sus pasos, y nos brinda informacion nueva de alta calidad y valor, ahora bien, solo hemos importado datos de un indicador específico y hemos graficado el comportamiento de dicho indicador **(EPU)**, este indicador es poderoso, pero las variables aisladas son poco dicientes, de manera de ejemplo, observemos como se relaciona esta variable con otras variables de mercado que son importantes para percibir los riesgos de mercado en los escenarios Mark to Market. 
Ya esta claro el proceso de importacion de datos, es necesario escoger otras variables de mercado para ver como se relacionan entre si, para ello se han seleccionado, el **DOW JONES, SP500, VIX, WTI, Treasury_10y, CPI y Unemployment Rate**
El **Dow Jones** y **SP500** represtaran el comportamiento de la renta variable, al ser dos de los indices mas importantes de la economia,
El **VIX** mide los niveles de volatilidad conocido por sus siglas como **(Exchange Market Volatility Inde)** indice calculado por **Chicago Board Options**, el **Treasury_10y** se refiere a los bonos del Tesoro de Estados Unidos a 10 años, es decir un horizonte largoplacista, escogido para este ejemplo simplificado como indicador de la renta fija.
Los ultimos dos seleccionados **CPI** y **Unemployment Rate** son seleccionados como indicadores economicos de relevancia, el **Consume Price Index** es un indicador que da pistas sobre el comportamiento inflacionario, una economia que crece de manera sana, tiene niveles de inflacion controlados, al rededor del 2% porque con esto se presume que hay una demanda estable de bienes y servicios, gran parte de la demanda esta justificada por los niveles de ingreso de los ciudadanos, que a grosso modo es muy influido en promedio, por los niveles de empleo **UNRATE**, es decir que a niveles altos de empleo, lo normal es que se estimule el consumo, lo que a su vez estimula la economia. 

Con niveles de incertidumbre tan altos vale la pena observar estas correlaciónes junto al **USD** que mediremos utilizando el indice **DXY CUR** que es un indicador del valor relativo del dolar americano respecto a otras fiat de gran importancia en la economia global.

Para cumplir dicho objetivo el primer paso es la importación de los datos de interés que a partir del ejemplo anterior, es generalizable:

### Importación de Datos de Mercado: 

```{r market_data}
market_data <- tq_get(c("DJIA", "SP500", "VIXCLS", "DTWEXAFEGS",
                         "UNRATE", "CPIAUCSL", "DGS10"),
                     get = "economic.data", from = "2010-01-01")

market_data
# Datos de las variables seleccionadas desde el año 2010, "post Sub-prime Crisis" en el 2008
# Los datos están representados de forma Tidy.  
```



### Transformacion:

Con el fin de calcular la correlación de **Pearson**, necesitamos transformar los datos, es decir transponer por variable, para crear una matriz que nos permita crear la matriz de correlación, para ello operaremos sobre el objeto que ya contiene la información, los datos no se generan con la misma frecuencia, es decir los indicadores economicos se publican cada semana o pueden tener una temporalidad incluso mensual, sin embargo los precios de los activos, si que cotizan en el dia a dia, por lo que habrá momentos en el tiempo en los que no existan todos los datos, para solucionar el problema, asumiremos que los datos de baja frecuencia de publicación, permanencen constantes hasta que son actualizados,
ademas la matriz no debe contemplar fechas, puesto que no es un valor al que vayamos a aplicarle la correlación, ya es sabido por el primer paso la franja temporal analizada. 

```{r market_matrix}

market_matrix <- market_data %>%
  pivot_wider(names_from = symbol, values_from = price) %>% 
  na.locf() %>% 
  select(-date) 

market_matrix

# Con la matriz de precios depurada, el siguiente paso es correlar.

```


### Matriz
Para correlaciónar la matriz de precios historicos, solo hemos de aplicar la funcion cor(), se debe tener en cuenta que el coeficiente de correlación solo indica la direccion y fuerza de la correlación, pero no es indicador alguno de causalidad. 
la matriz de correlación calculada, ofrece mucho valor, sin embargo no es de interpretación intuitiva.

Para solucionar este problema, y que el esfuerzo por interpretar el resultado sea menor, inmediatamente se explicara como crear una visualización y clusterizado eficiente y de facil ejecución.


```{r corr_matrix}

corr_matrix <- market_matrix %>% cor()


corr_matrix %>% print()


#Como resultado podemos observar el coeficiente de correlación de los datos historicos de las variables de interes.


```


Ademas de que la matriz de correlación tiene las limitaciones previamente expresadas, tambien se debe tener en cuenta que el coeficiente presentado, no cambia con mucha facilidad, es decir esta calculado con base a información historica, pero la correlación puede ser un fenomeno más dinámico, es decir si bién el coefiente de correlación siempre se encuenta en el rango **[-1 : 1]** es posible que cambie en el corto y mediano plazo, es decir que la correlación oscile en el paso del tiempo. 

El coeficiente de correlación, se interpreta como: 

                                                   1 = Perfecta Correlación Positiva.
                                                  -1 = Perfecta Correlación Negativa.
                                                  
                                                  
Cuando la correlación es **1**, las variables analizadas se comportan la mayor parte del tiempo, en el mismo sentido, dirección y fuerza
cuando el valor es **-1**, las variables se comportan en sentido y dirección contraria, con la misma intensidad o fuerza.


### Visualización de Matriz de Correlación: 

Se puede apreciar, que los coeficientes de correlación, han sido ordenados, siguiendo un patrón de agrupamiento jerárquico, esto con el fin de mejorar la interpretabilidad.  

```{r matrix_corrplot}

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))


matrix_corrplot <- 
    corrplot(corr_matrix,
             method = "color",
             col = col(200),
             addCoef.col = "black",
             tl.col = "darkblue",
             order = "hclust")

```

Entonces bien, conocer en que sentido, dirección y con que intensidad se relacionan las variables de mercado es bastante interesante, pero que tal seria observar el comportamiento de esta relacion en el transcurso del tiempo ? 
para se puede aplicar la misma funcion de correlacion en distintas ventanas de tiempo, esta tecnica se conoce como **"Rolling Window"**

Claro esta que al ser dinamica seria muy laborioso visualizarla como una matriz, visualicemos una linea de evolución en el tiempo, despues de calcular esta correlacion dinamica de las mismas variables en cuestion pero respecto el **DXY CUR** como referencia, es decir si bien ya se tiene respuesta a la pregunta: 

¿Como se relacionan la variables entre sí ? 

A continuación buscaremos respuesta a la pregunta:

¿Como evoluciona la relacion de las variables en el tiempo? 

Se debe tener en cuenta que esta evolucion en el tiempo, sera respetando el rango permitido de correlación, es decir el máximo valor posible en el tiempo sera **+1** y el mínimo posible **-1** 

### Correlación Dinámica:

A diferencia de la matrix anterior, en ese ejercicio, se correlaciona las variaciones del precio no el precio como tal, es decir el retorno del activo o variación porcentual, el cociente entre el valor en el momento 1 y anterior. 

```{r market_daily_returns, warning=F}
# Retorno o variacion de las variables seleccionadas.

market_daily_returns <- market_data %>% 
  group_by(symbol) %>% 
  tq_transmute(select = price,
               mutate_fun = periodReturn,
               period = "daily")

market_daily_returns

# Retornos del dolar, DXY CUR, valor base "baseline"
baseline_daily <- "DTWEXAFEGS" %>%
  tq_get(get  = "economic.data",
         from = "2010-01-01",
         to   = Sys.Date()) %>%
  tq_transmute(select     = price,
               mutate_fun = periodReturn,
               period     = "daily")

baseline_daily

```

A continuación, una sola matriz que contenga ambos conjuntos de datos, significa que la correlación será de cada una de las variables respecto al Baseline que en este caso es: **DXY CUR**, para observar la correlación de dichas variables respecto al dollar en el transcurso del tiempo.

```{r joined}
returns_joined <- left_join(market_daily_returns,
                            baseline_daily,
                            by = "date")%>% 
  na.omit()

returns_joined
# Matriz unificada para correlar 
```

El siguiente paso es aplicar la formula de correlación a las variables ya ordenadas en el objeto anterior, para ello utilizaremos la funcion tq_transmute de tidyquant y la aplicaremos a ambas variables o columnas, la base : **DXY CUR** y el conjunto de variables. 

```{r rolling_corr}

#Calculate the correlation. 

rolling_corr <- returns_joined %>%
  tq_transmute_xy(x          = daily.returns.x,
                  y          = daily.returns.y,
                  mutate_fun = runCor,
                  n          = 6,
                  col_rename = "rolling.corr")

rolling_corr %>% 
  print(n = 25)

#Observese que los valores "NA" corresponden a la ventana de tiempo contemplada* 


```


### Visualización de Correlación Dinámica:


A continuación, despues de haber calculado la correlación, el ultimo paso para este analísis, es la visualización, para ello utilizaremos lineas y crearemos una division para cada uno de los objetos a visualizar.

```{r rolling_corr_plot, warning=FALSE}

rolling_corr_plot <- 
# ggplotly( 
  rolling_corr %>%
    filter(symbol != "DTWEXAFEGS",
           date>Sys.Date()-30) %>% 
    ggplot(aes(x = date, y = rolling.corr, color = symbol)) +
    geom_hline(yintercept = 0, color = palette_light()[[8]]) +
    geom_line(size = 1.5) +
    labs(title = "Rolling Correlation to DXY CUR",
         x = "", y = "Correlation", color = "") +
    facet_wrap(~ symbol, ncol = 2) +
    theme_tq() +
    scale_color_tq()
#)

rolling_corr_plot

# Comportamiento de la correlación para los ultimas 30 dias. 
```


Despues de haber analizado el comportamiento de la correlación y gracias a la importación de los datos sobre niveles de incertidumbre, podemos tener una clara intuición de que los niveles de riesgo deben ser altos en este momento , puesto que la inceridumbre esta explorando nuevos maximos, ahora bien que hay del mito que ente más riesgo mayor el beneficio o rentabilidad? 

importemos datos del **Dow Jones** y **Standard & Poors 500**, un indice importante conformado por las empresas mas representativas, y un portafolio bastante equilibrado y diversificado, despues comparemos el comportamiento  del riesgo vs el retorno en el paso del tiempo para cada elemento, **Dow Jones & SP500**. 


# Ratio de Riesgo vd Rentabilidad: 


```{r risk_reward_tbl, warning=FALSE}

# DOW JONES
dow_jones <-
  tq_index("DOW") %>%
  tq_get() %>%
  group_by(symbol) %>%
  tq_transmute(adjusted,
               mutate_fun = periodReturn,
               period = "daily",
               type = "log",
               col_rename = "dlr") %>%
  summarize(MDLR  = mean(dlr),
            SDDLR = sd(dlr))

dow_jones

# SP500
# sp500 <-
#   tq_index("SP500") %>%
#   tq_get() %>%
#   group_by(symbol) %>%
#   tq_transmute(adjusted,
#                mutate_fun = periodReturn,
#                period = "daily",
#                type = "log",
#                col_rename = "dlr") %>%
#   summarize(MDLR  = mean(dlr),
#             SDDLR = sd(dlr))
# 
# sp500

```


## Visualizacion de Ratio: 


```{r risk_reward_plot, warning=FALSE}

# Evaluating Risk vs Reward for Dow Jones & SP500 Stocks:

dow_jones %>%
  ggplot(aes(x = SDDLR, y = MDLR)) +
  geom_point(color = palette_light()[[1]], alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Evaluating Risk vs Reward for DOW Stocks") +
  theme_tq()


# sp500 %>%
#   ggplot(aes(x = SDDLR, y = MDLR)) +
#   geom_point(color = palette_light()[[1]], alpha = 0.5) +
#   geom_smooth(method = "lm") +
#   labs(title = "Evaluating Risk vs Reward for S&P500 Stocks") +
#   theme_tq()


```


Podemos deducir facilmente, que los mayores niveles de riesgo no necesariamente se relacionan con mayor rentabilidad esperada, mucho menos en escenarios como el actual donde la incertidumbre es tan alta. 

Observemos a continuación, el comportamiento historico del **SP500**, importando los datos y creando un grafico sencillo con regresión lineal.

```{r sp_ggplot, warning=FALSE}

sp_500   <- tq_get("SP500", get = "economic.data", from = "2010-01-01" )

sp_500 %>% arrange(desc(date)) %>% 
  print(n=25)

# tibble del comportamiento historico del SP500 

sp_ggplot <- 
      sp_500%>%
        ggplot(aes(x = date, y = price))+
        theme_tq()+
        geom_point(color = "steelblue", size = 2)+
        geom_line(color = "gray", size = 1)+
        geom_smooth(color = "orange", method = loess)+
        labs(x = "Date", y = "Price Evolution",
             title = paste0("Standard & Poors Lastt 10 years"))

sp_ggplot

# Evolución del precio historico

```


En casos como estos en los que poseemos tanta información, resulta prudente y necesario, cortar la serie temporal, es decir observar un periodo de tiempo que este mas acotado, para lograrlo podemos filtrar en el tiempo, para dotarlo de dinamismo, observaremos solo los ultimos 60 dias de manera automatica utilizando la fecha del sistema:  

```{r sp500_ts, warning=FALSE}

sp500_ts <- 
  sp_500 %>%  
     filter(date >= Sys.Date()-60) %>% 
        ggplot(aes(x = date, y = price))+
        theme_tq()+
        geom_point(color = "steelblue", size = 2)+
        geom_line(color = "gray", size = 1)+
        geom_smooth(color = "orange", method = loess)+
        labs(x = "Date", y = "Price Evolution",
             title = paste0("Standard & Poors Last 60 days"))

sp500_ts

# Comportamiento de los ultimos 60 dias hasta la fecha de ejecución 

today()

# Podemos observar que a pesar de la tendencia alcista que subyacente al SP500, estos ultimos 2 meses se ha estado precipitando agresivamente. 

```

Una vez el escenario macroeconomico mejore, la compra del SP500 no entrañara tanto riesgo, la pregunta esta en el timing, es decir cuando se darán las circunstancias para que el comportamiento mejore, lo aconsejable en el momwnto de ejecución de este reporte es la mesura y causión, sin embargo como proposito de seguir elaborando el ejercicio, haremos un pronóstico del comportamiento de los datos utilizando un recurso creado por el equipo de Data Science de Facebook que se llama Prophet, este paquete se especializa en el analísis de series temporales. 
Las series temporales son un conjunto de herramientas de gran ayuda en el analísis cuantitativo de este tipo de comportamientos, sin embargo su complejidad es alta y deben ser interpretadas como variables cuantitativas no como pronósticos exactos, porque no existen las predicciones perfectas, menos aun cuando los niveles de incertidumbre están tan elevados. 

Más información sobre esta herramienta se encuentra disponible en este link <https://facebook.github.io/prophet/>


## Prophet

```{r prophet, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics("prophet.png")
```

El primer paso es segmentar la serie temporal como lo hicimos antes, se debe tener en cuenta que cualquier modelo o por lo menos la mayoria de estos, tienen un margen de error inferior en momentos no muy alejados del momento inical, puesto que a mayor horizonte temporal masyor incertidumbre respecto al desarrollo del escenario. 
Se utilizara la serie anterior correspondiente al SP500 pero con datos del ultimpo año, el paquete es muy sencillo, solo utiliza dos variables, **ds** referente al tiempo y  **yhat** equivalente al valor a proyectar, por lo tanto, tomamos el objeto anterior y lo transformamos para poder utilizar las funciones de analísis.




```{r prophet_forecast}

# fitting the model by price
sp <- 
  sp_500 %>% 
  filter(date >= Sys.Date()-600) %>% 
  select(date, price) %>% 
  rename(ds = date, y = price) %>% 
    na.locf() %>% 
    print(n = 30)

sp
m <- prophet(sp)
m

# fitting the model by returns
sp_ret <- "SP500" %>%
  tq_get(get  = "economic.data",
         from = Sys.Date()-600,
         to   = Sys.Date()) %>%
  tq_transmute(select     = price,
               mutate_fun = periodReturn,
               period     = "daily") %>%
   rename(ds = date, y = daily.returns) %>%
    na.locf()

sp_ret
n <- prophet(sp_ret)


```

Una vez despues de ajustar el modelo, podemos conocer los outputs del mismo,y por ejemplo, crear una tibble o tabla con los valores proyectados en el futuro, en este caso estamos proyectando 
```{r forecast}

# Predict --- Prices

future <- make_future_dataframe(m, periods = 15)
future %>% tail(15)

forecast <- predict(m, future)
forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')] %>% 
  tail(20)

# Predict --- Returns

future_n <- make_future_dataframe(n, periods = 15)
future_n %>% tail(15)

forecast_n <- predict(n, future)
forecast_n[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')] %>%
  tail(20)

```

Se puede observar que ya se han obtenido los valores proyectados en el futuro, con un intervalo de posible error que aumenta a lo largo del tiempo, en la tibble, se pueden observar sistintas variables, **ds** que es el tiempo, **yhat** la variable proyectada en el tiempo, **yhat_lower** de la proyeccion, valor minimo probable y finalmente **yhat_upper** valor maximo probable.
La tabla con los valores proyectados es muy util para cualquier analísis que se desea hacer con mayor nivel de profundidad o detalle, por ejemplo para crear una alerta que notifique una proyeccion muy baja o alta. 
Puede ser herramientas que combinadas con conocimiento del negocio y entendimiento de la dinamica actual, contribuya a que los procesos de toma de decisiones sea más sofisticado.
A continuacion observaremos la visualización de los datos calculados, donde los valores proyectados, seran representados de color azul.
```{r forecast_plot}
plot(m, forecast)
# identificamos con lineas rojas los cambios de dirección en el precio
plot(m, forecast) + add_changepoints_to_plot(m)


#Proyectar retornos en lugar de precios:
plot(n, forecast_n)
# identificamos con lineas rojas los cambios de dirección en el retorno
plot(n, forecast_n) + add_changepoints_to_plot(n)
```

Debe tenerse en cuenta que los niveles de volatitlidad en los ultimos dias, hacen que la serie temporal tenga un comportamient muy erratico, lo que dificulta el proceso de analisis y proyección, para intentar atenuar el efecto, podriamos buscar mas puntos de datos en menos horizonte temporal, para ello la importacion de datos debe tener una mayor frecuencia, es decir, la importación tradicional es de cada dia al cierre, se intentara importar datos intraday, es decir, hora a hora, o a lo mejor cada diez minutos. 

Para importar datos con una frecuencia tan alta, fue necesario investigar de donde descargar los datos, de una fuente que sea confiable, segura y de calidad, lo suficientemente rapida para proveer el servicio, este tipo se servicios son pagos, sin embargo se puede acceder a funciones de prueba, a continuación exploramos **AlphaVantage**, <"https://www.alphavantage.co/"> un API que permite infomación historica y en tiempo real, de series de tiempo del mercado. 

Inicialmente se importaran los datos correspondientes al **SP500**, datos intraday con frecuencia de 1 minuto. 
para poder acceder a dicha informacion se necesita una llave de acceso al API esta llave es personal, y se puede generar en el link compartido previamente. 

A Continuación se comparte una vista de la data descargada en el momento de ejecusión para posteriormente dibujar los datos en el tiempo.
Es de esperarse, que el comportamiento se erratico, por el escenario o conyuntura actual, y la alta frecuencia. 
Conoce el servicio premium en <"https://www.alphavantage.co/premium/">

```{r intraday}
#key
av_api_key("K40OVQLGD2QIE4TO")

#SP500
sp_data <- c("SPX") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")

sp_data %>% print(n = 35)

```

### Visualización con datos Intraday. 

graficación de los datos intradía con regresión lineal de ggplot.

```{r intraday_forecast}

sp_data %>% 
      ggplot(aes(x = timestamp, y = open))+
      theme_tq()+
      geom_point(color = "steelblue", size = 2)+
      geom_line(color = "gray", size = 1)+
      geom_smooth(color = "orange", method = loess)+
      labs(x = "Date", y = "Price",
           title = "S&P Intraday")  

```

Esta serie temporal del S&P500 es intradia y por la representación grafica y la corta ventana temporal y alta frecuencia, a simple vista  no parece ser estacional ni tiene una tendencia clara. 

A continuacion aplicaremos el modelo anteriormente aplicado de la libreria Prophet, para ello se deben transformar los datos de forma tal que puedan ser interpretables por la librería.

Para que la serie tenga un comportamiento mas suavizado, se creara una columna llamada **adj** resultado del promedio del **open, close, high & low** en cada instante de tiempo. 

```{r intraday_prophet}

sp_data_ts <- sp_data %>% 
  group_by(timestamp) %>%  
  mutate(adj = mean(c(open,high,close,low))) %>% 
  select("timestamp", "adj") %>% 
  rename(ds = timestamp, y = adj) %>%
  na.locf() 

sp_data_ts


m <- prophet(sp_data_ts, daily.seasonality = "auto",
             fit = TRUE, seasonality.mode = "multiplicative",
             uncertainty.samples = 100)


# Predict --- Prices

future <- make_future_dataframe(m, periods = 20)
future %>% tail(15)

forecast <- predict(m, future)
forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')] %>% 
  tail(20)

plot(m, forecast)
# identificamos con lineas rojas los cambios de dirección en el precio
plot(m, forecast) + add_changepoints_to_plot(m)


```


Al leer el codigo detalladamente, se puede observar que el objeto que contiene la información, denominado **sp_data_ts**, es aquel que contiene los datos intradía, descargados con el API de AlphaVantager, ahora bien los datos, tienen una frecuencia de un minuto, es decir que cada fila es una observación de la variable minuto a minuto, y la columna, no incorpora más que un dia de información, esta frecuencia tan alta de importación de datos, genera un conflicto con las herramientas del packete **Prophet** y también con las herramientas de visualización, para slocuionarlo, se sugiere una transformacion de los datos en los que tomando la matriz original **sp_data_ts**, creamos una columna temporal que no es más que el conteo de las filas de la serie, esta es creada con el fin de modificar la variable temporal **ds** y añadir un dia a cada observación para que el modelo pueda funcionar y desplegar su output de manera exitosa.

```{r intraday_fix_prophet}

sp_data_ts <- sp_data_ts %>% 
  ungroup() %>%  
  mutate(ds = as.Date(ds),
         day = row_number(),
         ds = ds+day) %>% 
  select(-day)

sp_data_ts


m <- prophet(sp_data_ts, daily.seasonality = "auto",
             fit = TRUE, seasonality.mode = "multiplicative",
             uncertainty.samples = 100)



# Predict --- Prices

future <- make_future_dataframe(m, periods = 20)
future %>% tail(15)

forecast <- predict(m, future)
forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')] %>% 
  tail(20)

plot(m, forecast)
# identificamos con lineas rojas los cambios de dirección en el precio
plot(m, forecast) + add_changepoints_to_plot(m)

```

Finalmente al realizar las observaciones e implementaciones de modelo, este sugiere que según las ultimas observaciones, la tendencia al alza persistira, un ejercicio interesante sera observar el desarrollo de los mercados a ver cuando se recuperan de la crisis actual, en el momento que se perciba acercamiento de la recuperación y la economia empiece a responder a los estimulos economicos, es muy probable que se recupere el equity. 

El modelo y aplicativo o instrumento de la tesis, permite al usuario hacer este tipo de analísis con más variables, y un mayor nivel de interactividad entre estas. 

Este reporte dinámico, no constituye el instrumento de la investigación si no su socialización, el codigo para ejecutar y reproducir este documento con valores actuales al momento de ejecución, esta disponible en el siguiente repositorio, <"https://github.com/Aggarch/MQF-UAH">, lugar en el que también se encuentra disponible el código que compartiremos a continuación: 

Estte repositorio permite hacer el control de versiones del instrumento para que pueda ser sometido a la mejora continua. 

El codigo que se desplegara a continuación sigue la misma logica general, de los ejercicios desarrollados en este R Markdown. 

El lector los podrá ejecutar sin dificultad en su consola R y buscar sus propios **insights** de negocio, tener en cuenta que cualquiera de los pasos y lineas de codigo a continuación, cumple uno o más de los siguientes objetivos;

**Importar, Transformar, Modelar y/o Comunicar**

Y que se trata de una dinamica Iterativa. 

Codigo perfectamente reproducible si se cuentan con conocimientos muy basicos de la herramienta. 

### Documentación 

tidyverse:      <"https://cran.r-project.org/web/packages/tidyverse/index.html> 
lubridate:      <"https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html">
knitr:          <"https://cran.r-project.org/web/packages/knitr/index.html">
tidyquant       <"https://cran.r-project.org/web/packages/tidyquant/index.html">
xml2            <"https://cran.r-project.org/web/packages/xml2/index.html">
corrplot        <"https://cran.r-project.org/web/packages/corrplot/corrplot.pdf">
alphavantager   <"https://cran.r-project.org/web/packages/alphavantager/alphavantager.pdf">
plotly          <"https://cran.r-project.org/web/packages/plotly/index.html">
prophet         <"https://cran.r-project.org/web/packages/prophet/index.html">


```{r the_code, eval=F, echo=TRUE}

#### QuantApp MBA Andrés García & Sagith Amín  ####

# ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: ----

# Infrastructure :  tidyquant {timetk, xts, zoo, Qunatmod, TTR, PerformanceAnalytics}
# Data Sources :::  FRED, Bloomberg, Quandl, Morningstar, Yahoo Finance
# Data Since   :::  2010-01-01 {After Sub-prime cicle, to reduce RV}


# Vignettes :::

# browseVignetteas(package = "tidyquant")
# browseVignettes(package = "timetk")


# Thesis: Data treatment philosophy:
# 1.)   Tidy - Data: ####
# makes reference to the macroeconomic data and treasury rates in a tidy format: 
# Data tidyng phiosophy: Each Column represents a variable, each row show the values.
browseURL("https://vita.had.co.nz/papers/tidy-data.pdf")

# Displpay some tidyquant options :::

tq_mutate_fun_options()
tq_performance_fun_options()
# ... ----

# 2.) MACROECONOMICS & PRIMES ####
# Macroeconomics: 

# Code may be optimize, however I prefer to indicate it step by step

# One by Variable:

# GDP per capita is gross domestic product divided by population.
# GDP is the sum of gross value added by all resident
# producers in the economy plus any product taxes and minus any
# subsidies not included in the value of the products. It is calculated
# without making deductions for depreciation of fabricated assets or for
# depletion and degradation of natural resources.
# World Bank national accounts data, and OECD National Accounts data files.

# # It's easier to make only one call to the data using the tq_get(), however for clarification 
# # purposes, we prefer to create one object by each variable. : 

# Like : 
# Market <- tq_get(c("DTWEXM", "DEXJPUS", "DEXCAUS", "DEXUSAL",
#                    "DEXUSUK", "DEXUSEU", "DGS10", "FEDFUNDS", "DCOILWTICO", 
#                    "GOLDAMGBD228NLBM","DJIA","SP500"),
#                  get = "economic.data", from = "2006-01-01") %>% 
#   filter(year(date) >=  "2010-01-01") %>% 
#   pivot_wider(names_from = symbol, values_from = price) %>% 
#   na.locf() %>% 
#   rename(DXY_CUR = DTWEXM) 



# GDP stands for "Gross Domestic Product" and represents the total monetary value of all final goods and services
# produced (and sold on the market) tthis index make reference to the Gobal market.
Global_GDP <- tq_get("NYGDPPCAPKDWLD", get = "economic.data", from = "2017-01-01")

# Economic Policy Uncertainty Index: EPU
#browseURL("http://www.policyuncertainty.com/media/BakerBloomDavis.pdf")
Economic_Policy_Risk <- tq_get("USEPUINDXD", get = "economic.data", from = "2017-01-01")

# Global Economic Policy Uncertainty: 
#browseURL("https://www.nber.org/papers/w22740")
Global_Economic_Uncertainty <- tq_get("GEPUPPP", get = "economic.data", from = "2010-01-01")

# Trade Policy uncertainty Index
#browseURL("https://www.policyuncertainty.com/china_monthly.html")
Trade_Policy_Risk <- tq_get("CHNMAINLANDTPU", get = "economic.data", from = "2010-01-01")

# Monthly Supplpy of Houses: 
Houses_Month_supply <- tq_get("MSACSR", get = "economic.data", from = "2010-01-01") 

# Consumer price Index - Inflation measure
CPI <- tq_get("CPIAUCSL", get = "economic.data", from = "2010-01-01", to = Sys.Date())

# Real Gross Domestic Prduct for US
Real_GDP <- tq_get("A191RL1Q225SBEA", get = "economic.data", from = "2010-01-01")

# Industrial Production Index
IPI <- tq_get("INDPRO", get = "economic.data", from = "2010-01-01") 

# Non-Farm Payrrols / Employment Measure
Non_Farm_Payrolls <- tq_get("PAYEMS", get = "economic.data", from = "2010-01-01")

# Unemployment Rate
Unemployment_Rate <- tq_get("UNRATE", get = "economic.data", from = "2010-01-01")
      
# US Treasury 10 Years
Treasury_10y <- tq_get("DGS10", get = "economic.data", from = "2010-01-01")

# US Treasury 2 Years 
Treasury_2y <-  tq_get("GS2", get = "economic.data", from = "2010-01-01")

# Reference interest Rates / setted by Central Bank> 
FED_Interest_Rate <- tq_get("FEDFUNDS", get = "economic.data", from = "2010-01-01")


# A ggplot of the Global Economic Policy Uncertainty : 
Economic_policy_index <- 
  ggplotly(
    Economic_Policy_Risk %>%
      filter(date >= "2019-10-20") %>% 
      ggplot(aes(x = date, y = price))+
      theme_tq()+
      geom_point(color = "steelblue", size = 2)+
      geom_line(color = "gray", size = 1)+
      geom_smooth(color = "orange", method = loess)+
      labs(x = "Date", y = "Economic Policy Uncertainty Index",
           title = "Global Economic Policy Uncertainty Evolution")
  )

# 2.1) Tidy tibble of Macro & Primes  ####
Macro_primes <- 
  bind_rows("Economic_Policy_Risk" = Economic_Policy_Risk,
            "Global_Uncertainty" = Global_Economic_Uncertainty,
            "Trade_Policy_Risk" = Trade_Policy_Risk,
            "CPI" = CPI,
            "Houses_Month_supply" = Houses_Month_supply,
            "IPI" = IPI, 
            "Non_Farm_Payrolls" = Non_Farm_Payrolls,
            "Unemployment_Rate" = Unemployment_Rate,
            "Treasury_10y" = Treasury_10y, 
            "Treasury_2y" = Treasury_2y, .id = "commodity_name")
Macro_primes

# 2.2) Correlation Macro & Primes  ####
# This correlation can be calculated after the data spread and matrix building 
Macros_Primes_corr <- 
  Macro_primes %>% 
  pivot_wider(names_from = commodity_name,values_from = price) %>% 
  na.locf() %>% 
  select(-date) %>% 
  cor() 
Macros_Primes_corr

# Correlation matrix ploted 
Macros_Primes_Corrplot <- 
  corrplot(Macros_Primes_corr, method = "shade" ,
           addCoef.col = "gray", 
           tl.col = "darkblue",
           order = "hclust",)
Macros_Primes_Corrplot

# Corrplot looks good at 80$ zoom

# ... ----

# 3.)  INDEXES ####

# 3.1) Composition of Indexes ####
SP500 <- tq_index("SP500")
DJI <- tq_index("DOW")


# 3.2)  Sectors Pareto ####
SP500_sectors <- 
  SP500 %>% 
  group_by(sector) %>%
  summarise(weight2 = sum(weight)) %>%
  arrange(desc(weight2)) %>% 
  mutate(Pareto = cumsum(weight2))

DJI_sectors <- 
  DJI %>%
  mutate(pareto = cumsum(weight)) %>%
  group_by(sector) %>%
  summarise(weight2 = sum(weight)) %>%
  arrange(desc(weight2)) %>%
  mutate(cumulat = cumsum(weight2))

# NASDAQ Inside ::.
# NDAQ <- tq_exchange("NASDAQ")
# 
# NDAQ_sectors <- 
#   NSDAQ%>%
#   group_by(sector) %>%
#   summarise(n = n() , Mcap2 = cumsum(market.cap)/sum(market.cap)) %>%
#   arrange(desc(Mcap2))

# 3.3) Historic of Indexes ####
SP500   <- tq_get("SP500", get = "economic.data", from = "2010-01-01" )
DOW     <- tq_get("DJIA", get = "economic.data", from = "2010-01-01" )
NASDAQ  <- tq_get("NASDAQCOM", get = "economic.data", from = "2010-01-01")
NIKKEI  <- tq_get("NIKKEI225", get = "economic.data", from = "2010-01-01" )  
VIX     <- tq_get("VIXCLS", get = "economic.data", from = "2010-01-01" )
DXY_CUR <- tq_get("DTWEXM", get = "economic.data",from = "2010-01-01") 


# 3.4) Tidy tibble of Indexes ####
Indexes <- bind_rows("DOW" = DOW, "SP500" = SP500,
                     "NDAQ" = NASDAQ, "NIKKEI" = NIKKEI,
                     "VIX" = VIX, "DXY" = DXY_CUR, .id = "Indexes")

Indexes <- bind_rows("DOW" = DOW, "SP500" = SP500,
                     "NDAQ" = NASDAQ, "NIKKEI" = NIKKEI,
                     "VIX" = VIX, "Economic_Policy_Risk" = Economic_Policy_Risk,
                     .id = "Indexes")

Indexes <- bind_rows("DOW" = DOW, "SP500" = SP500,
                     "NDAQ"= NASDAQ, "NIKKEI" = NIKKEI,
                     "VIX" = VIX, "EPU" = Economic_Policy_Risk,
                     "DXY" = DXY_CUR,
                     .id = "Indexes")

Indexes

# 3.5) Correlation of Indexes ####
# We can observe that the correlation between the positive returns of stocks against volatility its negative  
# It drives us to dive into the relationship between Risk & Return 

Indexes %>% 
  pivot_wider(names_from = Indexes, values_from = price) %>% 
  select(-date,-symbol) %>%  
  na.omit() %>% 
  cor() -> index_cor

corrplot(index_cor)

Index_Corrplot <- 
  corrplot(index_cor, method = "square",
           addCoef.col = "gray", 
           tl.col = "darkblue",
           order = "hclust")

# Whats the Mean correlation of this heatmap,
# It may be interesting to plot it as the intercept of a rolling corr. 
DXY_mean_cor <- mean(index_cor [,7:7])

# 3.6) Rolling Correlation ####

# 1) Calculate returns: (daily)
Index_daily_returns <- Indexes %>% 
  group_by(Indexes) %>% 
  tq_transmute(select = price,
               mutate_fun = periodReturn,
               period = "daily")%>% 
  filter(date >= "2019-09-15")

# 2) Baseline
baseline_returns_daily <- "DTWEXM" %>%
  tq_get(get  = "economic.data",
         from = "2019-09-15",
         to   = Sys.Date()) %>%
  tq_transmute(select     = price,
               mutate_fun = periodReturn,
               period     = "daily")

returns_joined <- left_join(Index_daily_returns,
                            baseline_returns_daily,
                            by = "date") %>% na.omit

# 3) Calculate the correlation. 
Index_rolling_corr <- returns_joined %>%
  tq_transmute_xy(x          = daily.returns.x,
                  y          = daily.returns.y,
                  mutate_fun = runCor,
                  n          = 6,
                  col_rename = "rolling.corr")

ggplotly( 
  Index_rolling_corr %>%
    filter(Indexes != "DXY") %>% 
    ggplot(aes(x = date, y = rolling.corr, color = Indexes)) +
    geom_hline(yintercept = c(DXY_mean_cor, 0), color = palette_light()[[8]]) +
    geom_line(size = 1.5) +
    labs(title = "Rolling Correlation to DXY",
         x = "", y = "Correlation", color = "") +
    facet_wrap(~ Indexes, ncol = 2) +
    theme_tq() +
    scale_color_tq()
)

# ... ----

# 4.)  STOCKS ####
AAPL  <- tq_get("AAPL", get = "stock.prices", from = "2010-01-01")
GOOG  <- tq_get("GOOG", get = "stock.prices", from = "2010-01-01")
MSFT  <- tq_get("MSFT", get = "stock.prices", from = "2010-01-01")
AMZN  <- tq_get("AMZN", get = "stock.prices", from = "2010-01-01")
FB  <- tq_get("FB", get = "stock.prices", from = "2010-01-01")
TWTR  <- tq_get("TWTR", get = "stock.prices", from = "2010-01-01")

# 4.1) Tidy tibble of Stocks ####
Stocks <- bind_rows("AAPL" = AAPL, "GOOG" = GOOG, "MSFT" = MSFT,
                    "AMZN" = AMZN, "FB" = FB, "TWTR" = TWTR, .id = "id") 

# 4.2) Monthly price ####
Stocks %>% 
  group_by(id) %>% 
  tq_transmute(select = close, 
               mutate_fun = to.monthly,
               indexAt = "lastof")

# 4.3) Rollapply: ####
# Calculate returns 
Returns <- 
  Stocks %>% 
  group_by(id) %>% 
  tq_transmute(adjusted, 
               periodReturn, period = "weekly", 
               col_rename = "Returns")

apple_returns <- 
  AAPL %>% 
  tq_transmute(adjusted,
               periodReturn, period = "weekly",
               col_rename = "APPLE.returns")

fb_returns <- 
  FB %>% 
  tq_transmute(adjusted,
               periodReturn, period = "weekly",
               col_rename = "FB.returns")

# To blend returns or combine them calculate indicidually a join them.
blend_returns <- left_join(fb_returns, apple_returns, by = "date")

# Apply custom function
regr_fun <- function(data) {
  coef(lm(FB.returns ~ APPLE.returns, data = timetk::tk_tbl(data, silent = TRUE)))
}

# ... ---- 

# 5.) Rolling regression with rollapply ####
# Apply the custom function. 

blend_returns %>%
  tq_mutate(mutate_fun = rollapply,
            width      = 12,
            FUN        = regr_fun,
            by.column  = FALSE,
            col_rename = c("coef.0", "coef.1"))

# ... ----   

# 6.)  COMMODITIES ####

# Set category id for Oil and Gold.
WTI      <- tq_get("WTISPLC", get = "economic.data", from = "2010-01-01" )
Brent    <- tq_get("POILBREUSDM", get = "economic.data", from = "2010-01-01")
Gold     <- tq_get("GOLDAMGBD228NLBM", get = "economic.data", from = "2010-01-01")
Aluminum <- tq_get("PALUMUSDM", get = "economic.data", from = "2010-01-01")
#Copper   <- tq_get("PCOPPUSDM", get = "economic.data", from = "2010-01-01")
Corn     <- tq_get("PMAIZMTUSDM", get = "economic.data", from = "2010-01-01") 
Soy      <- tq_get("PSOYBUSDQ", get = "economic.data", from = "2010-01-01")

# 6.1) Tidy tibble of Commodities ####
Commodities <- 
  bind_rows("WTI" = WTI, "Brent" = Brent,
            "Gold" = Gold, "Aluminum" = Aluminum, 
            "Corn" = Corn, "Soy" = Soy,
            .id = "commodity_name")
Commodities


# 6.2) Commodities Correlation ####
Commodities_cor <- 
  Commodities %>% 
  pivot_wider(names_from = commodity_name,values_from = price) %>% 
  na.locf() %>% 
  select(-date) %>% 
  cor() 
Commodities_cor

#Quaterly
Commodities_Corrplot <- 
  corrplot(Commodities_cor, method = "square",
           addCoef.col = "gray", 
           tl.col = "darkblue",
           order = "hclust")
Commodities_Corrplot

# ... ----

# 7.) CURRENCIES ####
# Pricipal currencies in the world:
# browseURL("https://www.investopedia.com/trading/most-tradable-currencies/")
# Dollar Basket:
# browseURL("https://www.bloomberg.com/quote/DXY:CUR")

#Dollar Basket:
# DXY_CUR <- tq_get("DTWEXM", get = "economic.data",from = "2010-01-01")  


DXY_CUR <- tq_get("DTWEXAFEGS", get = "economic.data",from = "2010-01-01") 
EUR_USD <- tq_get("DEXUSEU", get = "economic.data", from = "2010-01-01")
GBP_USD <- tq_get("DEXUSUK", get = "economic.data", from = "2010-01-01")
USD_JPY <- tq_get("DEXJPUS", get = "economic.data", from = "2010-01-01")
USD_CAD <- tq_get("DEXCAUS", get = "economic.data", from = "2010-01-01")
AUD_USD <- tq_get("DEXUSAL", get = "economic.data", from = "2010-01-01")
USD_CHF <- tq_get("DEXSZUS", get = "economic.data", from = "2010-01-01")
USD_SEK <- tq_get("DEXSDUS", get = "economic.data", from = "2010-01-01")

# Primes are upside in Macroeconomics 

FX  <- bind_rows("DXY:CUR" = DXY_CUR,
                 "EUR/USD" = EUR_USD,  "GBP/USD" = GBP_USD,
                 "USD/JPY" = USD_JPY,  "USD/CAD" = USD_CAD,
                 "AUD/USD" = AUD_USD,  "USD/CHF" = USD_CHF,
                 "USD/SEK" = USD_SEK, .id = "currency")

FX

# 7.1) Tidy tibble of Fx + primes ####
FX_primes <- bind_rows("DXY:CUR" = DXY_CUR, 
                       "EUR/USD" = EUR_USD,  "GBP/USD" = GBP_USD,
                       "USD/JPY" = USD_JPY,  "USD/CAD" = USD_CAD,
                       "AUD/USD" = AUD_USD,  "USD/CHF" = USD_CHF,
                       "ZAR/USD"=ZAR_USD, "FED_Interest_Rate" = FED_Interest_Rate, 
                       "Treasury_10y" = Treasury_10y, "Treasury_2y" = Treasury_10y,
                       .id = "currency") 
FX_primes

# 7.2) Correlate the F Exchange ####
FX_primes_cor <- 
  FX_primes %>% 
  pivot_wider(names_from = currency, values_from = price) %>% 
  select(-date) %>% 
  na.omit() %>% 
  cor() 


FX_Primes_Corrplot <- 
  corrplot(FX_primes_cor, method = "square",
           addCoef.col = "gray", 
           tl.col = "darkblue",
           order = "hclust")

# ... ----

# 8) DATA LAKE TABLE xlsx ####
# Data lost cause bind_rows step, 
# Full Data coverage, do data frames really affect the result ?  

# FX <- 
# DXY_CUR %>% 
#        left_join(EUR_USD, by = c("date" = "date")) %>% 
#             left_join(GBP_USD, by = c("date" = "date")) %>% 
#                left_join(USD_JPY, by = c("date" = "date")) %>% 
#                    left_join(USD_CAD, by = c("date" = "date")) %>% 
#                        left_join(AUD_USD, by = c("date" = "date")) %>% 
#                           left_join(USD_CHF, by = c("date" = "date")) %>% 
#                              left_join(ZAR_USD, by = c("date" = "date")) %>%                          
#   
#        rename(DXY_CUR = price.x, EUR_USD = price.y, GBP_USD = price.x.x, USD_JPY = price.y.y,
#               USD_CAD = price.x.x.x, AUD_USD = price.y.y.y, USD_CHF = price.x.x.x.x, ZAR_USD = price.y.y.y.y ) %>% 
#   na.locf()



# Macroeconomics 
MACROS <- 
  Macro_primes %>% 
  pivot_wider(names_from = commodity_name,values_from = price) %>% 
  na.locf()
MACROS

# Indexes 
INDEX <- 
  Indexes %>% 
  pivot_wider(names_from = Indexes, values_from = price) %>% 
  na.locf()
INDEX

# Commodities 
COMMODITIES <- 
  Commodities %>% 
  pivot_wider(names_from = commodity_name,values_from = price) %>% 
  na.locf()
COMMODITIES

# FX exchanges   
FX <- 
  FX %>% 
  pivot_wider(names_from = currency, values_from = price) %>% 
  na.locf()
FX

# FX exchanges and Primes in one tibble 
FX_PRIMES <- 
  FX_primes %>% 
  pivot_wider(names_from = currency, values_from = price) %>% 
  na.locf()
FX_PRIMES

# Global jointtt
GLOBAL_MARKET_RISK <- 
  MACROS %>% 
  left_join(INDEX, by = c("date" = "date")) %>% 
  left_join(COMMODITIES, by = c("date" = "date")) %>% 
  left_join(FX, by = c("date" = "date")) %>% 
  na.locf()
GLOBAL_MARKET_RISK

#install.packages("openxlsx")

# Lets save our datasets in a easy to read Excel tables, verify the Working Directory.  

getwd()
setwd("C:/Users/ANALITICA/Documents/Thesis DataFrames")

library(openxlsx)

wb <- createWorkbook()
addWorksheet(wb = wb, sheetName = "FX", gridLines = TRUE)
writeDataTable(wb = wb, sheet = 1, x = FX)

openxlsx::write.xlsx(MACROS,             "1.) MACROECONOMICS.xlsx", asTable = TRUE, sheetName = "MACROECONOMICS")
openxlsx::write.xlsx(INDEX,              "2.) INDEX.xlsx", asTable = TRUE, sheetName = "Index")
openxlsx::write.xlsx(COMMODITIES,        "3.) COMMODITIES.xlsx", asTable = TRUE, sheetName = "Commodities")
openxlsx::write.xlsx(FX,                 "4.) FX.xlsx", asTable = TRUE, sheetName = " Currencies")
openxlsx::write.xlsx(FX_PRIMES,          "5.) FX_PRIMES.xlsx", asTable = TRUE, sheetName = "FX and Primes")
openxlsx::write.xlsx(GLOBAL_MARKET_RISK, "6.) GLOBAL_MARKET_RISK.xlsx", asTable = TRUE, sheetName = "Global_Market_Risk") 
# Nuestro pool de datos es de 3578 registros sobre 31 variables. 


# To succesfully combine the 3 assets cathegories into one tidy data tibble, 
# the id label must be the same, 
# then we can spread the data and correlate. 

# GLOBAL_MARKET_RISK 
# Already Tidy 
GLOBAL_MARKET_RISK 

Global_cor <- 
  GLOBAL_MARKET_RISK %>% 
  select(-date) %>% 
  na.locf() %>% 
  cor() 


# Correlation of GLOBAL_MARKET 
Global_market_Corrplot <-  
  corrplot(Global_cor, method = "square",
           tl.col = "darkblue",
           order = "hclust")
Global_market_Corrplot

# ... ----

# 9) PORTFOLIO ####

#### time Series ####

library(prophet)
library(tidyverse)
library(lubridate)

setwd("C:/Users/ANALITICA/Desktop")

DXY_CUR <- tq_get("DTWEXM", get = "economic.data",from = "2017-01-01") %>% 
  select(date, price) %>% 
  rename(ds = date, y = price)

m <- prophet(DXY_CUR)

future <- make_future_dataframe(m, periods = 95)
tail(future)

forecast <- predict(m, future)
tail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])


# R
ggplotly( 
  plot(m, forecast)
)

# plotting with dygraphs
library(dygraphs) 
dyplot.prophet(m, forecast)


# Series Components
prophet_plot_components(m, forecast)

# Add checkpoints 
plot(m, forecast) + add_changepoints_to_plot(m)

# R
m <- prophet(DXY_CUR, changepoint.prior.scale = 0.5)
forecast <- predict(m, future)
plot(m, forecast)


#### Sources ::::::::::::::::::::::::::::::::::::::::::::::::::::: ####

#### Simple Portfolio #### 

stock_returns_monthly <- c("AAPL", "GOOG", "NFLX") %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "Ra")
stock_returns_monthly

baseline_returns_monthly <- "XLK" %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "Rb")
baseline_returns_monthly

wts <- c(0.5, 0.0, 0.5)
portfolio_returns_monthly <- stock_returns_monthly %>%
  tq_portfolio(assets_col  = symbol, 
               returns_col = Ra, 
               weights     = wts, 
               col_rename  = "Ra")
portfolio_returns_monthly

wts_map <- tibble(
  symbols = c("AAPL", "NFLX"),
  weights = c(0.5, 0.5)
)
wts_map

stock_returns_monthly %>%
  tq_portfolio(assets_col  = symbol, 
               returns_col = Ra, 
               weights     = wts_map, 
               col_rename  = "Ra_using_wts_map")

RaRb_single_portfolio <- left_join(portfolio_returns_monthly, 
                                   baseline_returns_monthly,
                                   by = "date")
RaRb_single_portfolio

portfolio_returns_monthly %>%
  ggplot(aes(x = date, y = Ra)) +
  geom_bar(stat = "identity", fill = palette_light()[[1]]) +
  labs(title = "Portfolio Returns",
       subtitle = "50% AAPL, 0% GOOG, and 50% NFLX",
       caption = "Shows an above-zero trend meaning positive returns",
       x = "", y = "Monthly Returns") +
  geom_smooth(method = "lm") +
  theme_tq() +
  scale_color_tq() +
  scale_y_continuous(labels = scales::percent)

wts <- c(0.5, 0, 0.5)
portfolio_growth_monthly <- stock_returns_monthly %>%
  tq_portfolio(assets_col   = symbol, 
               returns_col  = Ra, 
               weights      = wts, 
               col_rename   = "investment.growth",
               wealth.index = TRUE) %>%
  mutate(investment.growth = investment.growth * 10000)

portfolio_growth_monthly %>%
  ggplot(aes(x = date, y = investment.growth)) +
  geom_line(size = 2, color = palette_light()[[1]]) +
  labs(title = "Portfolio Growth",
       subtitle = "50% AAPL, 0% GOOG, and 50% NFLX",
       caption = "Now we can really visualize performance!",
       x = "", y = "Portfolio Value") +
  geom_smooth(method = "loess") +
  theme_tq() +
  scale_color_tq() +
  scale_y_continuous(labels = scales::dollar)


monthly_returns_stocks <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(adjusted, periodReturn, period = "monthly")

weights <- c(0.50, 0.25, 0.25, 0)
tq_portfolio(data = monthly_returns_stocks,
             assets_col = symbol,
             returns_col = monthly.returns,
             weights = weights,
             col_rename = NULL,
             wealth.index = FALSE)



#### Multi Portfolio ####

stock_returns_monthly_multi <- stock_returns_monthly %>%
  tq_repeat_df(n = 3)
stock_returns_monthly_multi


weights <- c(
  0.50, 0.25, 0.25,
  0.25, 0.50, 0.25,
  0.25, 0.25, 0.50
)
stocks <- c("AAPL", "GOOG", "NFLX")
weights_table <-  tibble(stocks) %>%
  tq_repeat_df(n = 3) %>%
  bind_cols(tibble(weights)) %>%
  group_by(portfolio)
weights_table


portfolio_returns_monthly_multi <- stock_returns_monthly_multi %>%
  tq_portfolio(assets_col  = symbol, 
               returns_col = Ra, 
               weights     = weights_table, 
               col_rename  = "Ra")
portfolio_returns_monthly_multi

RaRb_multiple_portfolio <- left_join(portfolio_returns_monthly_multi, 
                                     baseline_returns_monthly,
                                     by = "date")
RaRb_multiple_portfolio


portfolio_growth_monthly_multi <- stock_returns_monthly_multi %>%
  tq_portfolio(assets_col   = symbol, 
               returns_col  = Ra, 
               weights      = weights_table, 
               col_rename   = "investment.growth",
               wealth.index = TRUE) %>%
  mutate(investment.growth = investment.growth * 10000)


portfolio_growth_monthly_multi %>%
  ggplot(aes(x = date, y = investment.growth, color = factor(portfolio))) +
  geom_line(size = 2) +
  labs(title = "Portfolio Growth",
       subtitle = "Comparing Multiple Portfolios",
       caption = "Portfolio 3 is a Standout!",
       x = "", y = "Portfolio Value",
       color = "Portfolio") +
  geom_smooth(method = "loess") +
  theme_tq() +
  scale_color_tq() +
  scale_y_continuous(labels = scales::dollar)


# Analize the SP500 Index Portfoloio Returns. 

SP500_returns <- SP500 %>%
  tq_transmute(select = price,
               mutate_fun = periodReturn,
               period = "monthly",
               col_rename = "Ra")


# All Weather : ####

#1.0 IMPORT DATA ----
symbols <- c("VTI", "TLT", "IEF", "GLD", "DBC")
end <- "2019-06-30" %>% ymd()
start <- end - years(30) + days(1)

raw_data <- symbols %>% 
  tq_get(get = "stock.prices",
         from = start,
         to = end)

raw_data %>% 
  group_by(symbol) %>% 
  summarise(min_date = min(date), 
            max_date = max(date))

# 2.0 TRANSFORM TO RETURNS ----
# normal returns
returns_reg_tbl <- raw_data %>% 
  select(symbol, date, adjusted) %>% 
  group_by(symbol) %>% 
  tq_transmute(select = adjusted,
               mutate_fun = periodReturn,
               period = "monthly") %>% 
  ungroup() %>% 
  
  #rollback to first day of the month - ETF Issue ----
mutate(date = lubridate::rollback(date, roll_to_first = TRUE))

raw_data %>% 
  select(symbol, date, adjusted) %>% 
  group_by(symbol) %>% 
  tq_transmute(select = adjusted,
               mutate_fun = periodReturn,
               period = "monthly") %>% 
  ungroup() %>% 
  spread(symbol, monthly.returns) %>% 
  na.omit()

returns_reg_date_tbl <- returns_reg_tbl %>% 
  group_by(symbol) %>% 
  filter(date >= "2006-02-01")

wts_tbl <- c(0.3, 0.4, 0.15, 0.075, 0.075)

returns_port_tbl <- returns_reg_date_tbl %>% 
  tq_portfolio(assets_col = symbol,
               returns_col = monthly.returns,
               weights = wts_tbl,
               rebalance_on = "years") %>% 
  add_column(symbol = "Portfolio", .before = 1) %>% 
  rename(monthly.returns = portfolio.returns)
end_port_date <- last(returns_port_tbl$date)

returns_port_tbl

# Weights Matrix ####
w_2 <- c(0.3, 0.4, 0.15, 0.075, 0.075,
         1, 0, 0, 0, 0,
         0, 1, 0, 0, 0,
         0, 0, 1, 0, 0,
         0, 0, 0, 1, 0,
         0, 0, 0, 0, 1)

weights_tbl <- tibble(symbols) %>% 
  tq_repeat_df(n = 6) %>% 
  bind_cols(tibble(w_2)) %>% 
  group_by(portfolio)

allocation <- weights_tbl %>%   
  ungroup() %>% 
  mutate(w_2 = paste0(w_2*100, "%")) %>% 
  pivot_wider(names_from = symbols, values_from = w_2) %>% 
  mutate(portfolio = case_when(portfolio == 1 ~ "All Seasons Portfolio",
                               portfolio == 2 ~ "VTI",
                               portfolio == 3 ~ "TLT",
                               portfolio == 4 ~ "IEF",
                               portfolio == 5 ~ "GLD",
                               portfolio == 6 ~ "DBC")) 


##### Multi Returns & Potfolio 

returns_multi_reg_date_tbl <- returns_reg_date_tbl %>% 
  ungroup() %>% 
  tq_repeat_df(n = 6)

port_returns_investment_tbl <- returns_multi_reg_date_tbl %>% 
  tq_portfolio(assets_col = symbol,
               returns_col = monthly.returns,
               weights = weights_tbl,
               wealth.index = TRUE) %>% 
  mutate(investment.growth = portfolio.wealthindex * 10000)

end_port_returns_investment_tbl <- last(port_returns_investment_tbl$date)

# ready to graph. 


# Tech small portfolio: #### 

Ra <- c("AAPL", "GOOG", "NFLX") %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,  
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "Ra")
Ra

# Lets Compare the returns above, wth the XLK SPDR 

Rb <- "XLK" %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "Rb")
Rb

# Combine the wth a left join. 

RaRb <- left_join(Ra, Rb, by = c("date" = "date"))
RaRb

# Finally, we can retrieve the performance metrics
# Use tq_performance_fun_options() to see the list of compatible performance functions.

RaRb_capm <- RaRb %>% 
  tq_performance(Ra = Ra,
                 Rb = Rb,
                 performance_fun = table.CAPM)
RaRb_capm

# We can isolate attributes, such as alpha = measure of growth & beta, risk measure. 

RaRb_capm %>% 
  select(Alpha, Beta)

# 1A ####
# Get Stock prices .:: 

stock_prices <- 
  c("AAPL", "GOOG", "NFLX") %>% 
  tq_get(get = "stock.prices",
         from = "2010-01-01",
         to = today())
stock_prices

# 2A ####
# Mutate to Returrns :.:

# monthly ...:::
stock_returns_monthly <- 
  stock_prices %>% 
  tq_transmute(select = adjusted, 
               mutate_fun = periodoReturn,
               period = "monthly",
               col_rename = "Ra")

# daily ...::: 
stock_retunrs_daily <- 
  stock_prices %>% 
  tq_transmute( select = adjusted,
                mutate_fun = periodReturn, 
                period = "daily",
                col_rename = "Ra")

# 3A #### 
# Analyze Performance ::.

stock_retunrs_monthly %>% 
  tq_performance(Ra = Ra, 
                 Rb= NULL,
                 performance_fun = SharpeRatio)



# EDA

netflix <- tq_get("NFLX", get = "stock.prices")

# sp_500 <- tq_index("SP500") %>%
#           tq_get(get = "stock.prices")

sp_500 <- tq_index("SP500") %>%
  tq_get(get = "stock.prices") %>%
  group_by(symbol) %>%
  filter(year(date) >= 2017)


# Monthly Returns::
sp_500 %>%
  group_by(symbol) %>%
  tq_transmute(adjusted,mutate_fun = monthlyReturn)

sp_500 <-
  tq_index("SP500") %>%
  tq_get() %>%
  group_by(symbol) %>%
  tq_transmute(adjusted,
               mutate_fun = periodReturn,
               period = "daily",
               type = "log",
               col_rename = "dlr") %>%
  summarize(MDLR  = mean(dlr),
            SDDLR = sd(dlr))

# Evaluating Risk vs Reward for S&P500 Stocks:
sp_500 %>%
  ggplot(aes(x = SDDLR, y = MDLR)) +
  geom_point(color = palette_light()[[1]], alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Evaluating Risk vs Reward for S&P500 Stocks") +
  theme_tq()


# Yearly Returns :::
FANG_annual_returns <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(select = adjusted,
               mutate_fun = periodReturn,
               period = "yearly",
               type = "arithmetic")

FANG_anual_returns


# FANG Annual Returns:

FANG_annual_returns %>%
  ggplot(aes(x = date, y = yearly.returns, fill = symbol)) +
  geom_col() +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "FANG: Annual Returns",
       subtitle = "Get annual returns quickly with tq_transmute!",
       y = "Annual Returns", x = "") +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  theme_tq() +
  scale_fill_tq()

#  FANG Daily Log returns:

FANG_daily_log_returns <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "daily",
               type       = "log",
               col_rename = "monthly.returns")


# Plotting log Returns:

FANG_daily_log_returns %>%
  ggplot(aes(x = monthly.returns, fill = symbol)) +
  geom_density(alpha = 0.5) +
  labs(title = "FANG: Charting the Daily Log Returns",
       x = "Monthly Returns", y = "Density") +
  theme_tq() +
  scale_fill_tq() +
  facet_wrap(~ symbol, ncol = 2)


FANG %>%
  group_by(symbol) %>%
  tq_transmute(select     = open:volume,
               mutate_fun = to.period,
               period     = "months")

# Daily stock prices:

FANG_daily <- FANG %>%
  group_by(symbol)

FANG_daily %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line(size = 1) +
  labs(title = "Daily Stock Prices",
       x = "", y = "Adjusted Prices", color = "") +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  scale_y_continuous(labels = scales::dollar) +
  theme_tq() +
  scale_color_tq()

# Monthly stock prices:

FANG_monthly <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = to.period,
               period     = "months")

FANG_monthly %>%
  ggplot(aes(x = date, y = adjusted, color = symbol)) +
  geom_line(size = 1) +
  labs(title = "Monthly Stock Prices",
       x = "", y = "Adjusted Prices", color = "") +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  scale_y_continuous(labels = scales::dollar) +
  theme_tq() +
  scale_color_tq()

# Asset Returns

---------------------------------------
  
  FANG_returns_monthly <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "monthly")

# Baseline Returns
baseline_returns_monthly <- "XLK" %>%
  tq_get(get  = "stock.prices",
         from = "2016-01-01",
         to   = Sys.Date()) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "monthly")

returns_joined <- left_join(FANG_returns_monthly,
                            baseline_returns_monthly,
                            by = "date")
returns_joined



# Rolling Correlations: !!!! #### 

FANG_rolling_corr <- returns_joined %>%
  tq_transmute_xy(x          = monthly.returns.x,
                  y          = monthly.returns.y,
                  mutate_fun = runCor,
                  n          = 6,
                  col_rename = "rolling.corr.6")

FANG_rolling_corr %>%
  ggplot(aes(x = date, y = rolling.corr.6, color = symbol)) +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  geom_line(size = 1.5) +
  labs(title = "FANG: Six Month Rolling Correlation to XLK",
       x = "", y = "Correlation", color = "") +
  facet_wrap(~ symbol, ncol = 2) +
  theme_tq() +
  scale_color_tq()

------------------------------------------------------
  
  # Moving Average Convergence Divergence (MACD)
  
  FANG_macd <- FANG %>%
  group_by(symbol) %>%
  tq_mutate(select     = close,
            mutate_fun = MACD,
            nFast      = 12,
            nSlow      = 26,
            nSig       = 9,
            maType     = SMA) %>%
  mutate(diff = macd - signal) %>%
  select(-(open:volume))
FANG_macd


FANG_macd %>%
  filter(date >= as_date("2016-10-01")) %>%
  ggplot(aes(x = date)) +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  geom_line(aes(y = macd, col = symbol)) +
  geom_line(aes(y = signal), color = "blue", linetype = 2) +
  geom_bar(aes(y = diff), stat = "identity", color = palette_light()[[1]]) +
  facet_wrap(~ symbol, ncol = 2, scale = "free_y") +
  labs(title = "FANG: Moving Average Convergence Divergence",
       y = "MACD", x = "", color = "") +
  theme_tq() +
  scale_color_tq()

# by quater

FANG_max_by_qtr <- FANG %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = apply.quarterly,
               FUN        = max,
               col_rename = "max.close") %>%
  mutate(year.qtr = paste0(year(date), "-Q", quarter(date))) %>%
  select(-date)
FANG_max_by_qtr

# Visualize

FANG_by_qtr %>%
  ggplot(aes(x = year.qtr, color = symbol)) +
  geom_segment(aes(xend = year.qtr, y = min.close, yend = max.close),
               size = 1) +
  geom_point(aes(y = max.close), size = 2) +
  geom_point(aes(y = min.close), size = 2) +
  facet_wrap(~ symbol, ncol = 2, scale = "free_y") +
  labs(title = "FANG: Min/Max Price By Quarter",
       y = "Stock Price", color = "") +
  theme_tq() +
  scale_color_tq() +
  scale_y_continuous(labels = scales::dollar) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.title.x = element_blank())

# Get stock pairs
stock_prices <- c("MA", "V") %>%
  tq_get(get  = "stock.prices",
         from = "2015-01-01",
         to   = "2016-12-31") %>%
  group_by(symbol)

stock_pairs <- stock_prices %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "daily",
               type       = "log",
               col_rename = "returns") %>%
  spread(key = symbol, value = returns)

stock_pairs %>%
  ggplot(aes(x = V, y = MA)) +
  geom_point(color = palette_light()[[1]], alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Visualizing Returns Relationship of Stock Pairs") +
  theme_tq()


# Stock Monthly Returns

stock_returns_monthly <- c("AAPL", "GOOG", "NFLX") %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "monthly",
               col_rename = "Ra")


baseline_returns_monthly <- "XLK" %>%
  tq_get(get  = "stock.prices",
         from = "2010-01-01",
         to   = "2015-12-31") %>%
  tq_transmute(select     = adjusted,
               mutate_fun = periodReturn,
               period     = "monthly",
               col_rename = "Rb")
baseline_returns_monthly

# weights Matrix

wts <- c(.5, 0, .5)
portfolio_returns_monthly <- stock_returns_monthly %>%
  tq_portfolio(assets_col   = symbol,
               returns_col  = Ra,
               weights = wts,
               col_rename = "Ra")
portfolio_returns_monthly

wts_map <- tibble(
  symbols = c("AAPL", "NFLX"),
  weights = c(0.5, 0.5)
)
wts_map


# 10) Alternative Data Source #### 
#........####

# Alpha Vantage Data: ####
# To get Market data in Alpha: 

# install.packages("alphavantager")
# browseURL("https://www.alphavantage.co/documentation/")
# alphak = "K40OVQLGD2QIE4TO"

av_api_key("K40OVQLGD2QIE4TO")

# aplicable to currencies
my_intraday_data <- c("FB", "MSFT") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")

eur_usd <- c("EURUSD") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")

my_intraday_data <- c("FB", "MSFT") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_DAILY_ADJUSTED")



#Intraday data for indexes: 

#SP500
sp_data <- c("SPX") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")

#DOW JONES
dow_data <- c("DJIA") %>%
  tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")

#NDAQ ?? 
# my_intraday_data <- c("COMP") %>%
#   tq_get(get = "alphavantager", av_fun = "TIME_SERIES_INTRADAY", interval = "1min")


EURUSD <- tq_get(get = "alphavantager", av_fun = "CURRENCY_EXCHANGE_RATE",
                 from_currency=USD, to_currency=EUR, interval = "30min", 
                 outputsize = compact, apikey = "K40OVQLGD2QIE4TO")




# SMA
av_get(symbol = "EURUSD", av_fun = "SMA", interval = "5min", time_period = 60, series_type = "close") %>% print(n=100)



# INTRADAY DATA 
# output size = "compact" # For daily data.
# interval = "1min"

# option available also for stocks 

setDefaults(getSymbols.av, api.key="K40OVQLGD2QIE4TO")

getSymbols("EURUSD",
           src="av",
           api.key="K40OVQLGD2QIE4TO",
           output.size="full",
           periodicity="intraday")

getSymbols("GBPUSD",
           src="av", 
           api.key="K40OVQLGD2QIE4TO",
           output.size="full",
           periodicity="intraday")

getSymbols("GBPUSD",
           src = "av",
           api.key = "K40OVQLGD2QIE4TO", 
           output.size = "full",
           periodicity = "daily")

EURUSD %>% tail(); GBPUSD %>% tail()

browseURL( "https://www.rdocumentation.org/packages/quantmod/versions/0.4-15/topics/getSymbols.av")

# More Data sources: ####
# browseURL("https://towardsdatascience.com/free-financial-data-a122a3cd5531")
# browseURL("https://www.openquants.com")


# SEC ####
install.packages("finreportr")
library(finreportr)

#General info :
AAPL.Info<-CompanyInfo("AAPL")
#Reports accesion: 
AAPL.reports<-AnnualReports("AAPL")

# Essentially Company financials are organized into 3 segments: 
# Income statement, Balance sheet & Cash flow: 
AAPL.IS <- GetIncome("AAPL", 2017)

#Metrics:
unique(AAPL.IS$Metric)

# Balance Sheet: 
AAPL.BS<-GetBalanceSheet("AAPL", 2017) 

# Cash Flow: 
AAPL.CF<-GetCashFlow("AAPL", 2018)

# Quandl ####
# browseURL("https://www.quandl.com/")

install.packages("Quandl")
library(Quandl)

Quandl.api_key("5qWrGohi2Gx16o4zG-o7")
from.dat <- as.Date("01/01/2010", format="%d/%m/%Y")
to.dat <- as.Date("01/01/2019", format="%d/%m/%Y")
crude.oil.futures<-Quandl("CHRIS/CME_CL1", start_date = from.dat, end_date = to.dat, type="xts")
plot(crude.oil.futures$Last)




gold.futures<-Quandl("CHRIS/CME_GC6", start_date = from.dat, end_date = to.dat, type="xts")
plot(gold.futures$Last)


gold_futures <- 
  Quandl("CHRIS/CME_GC6", api_key="5qWrGohi2Gx16o4zG-o7")

fred.GDP <- Quandl("FRED/GDP", type = "xts")
plot(fred.GDP)

# Slice & Dice: 
GDP = Quandl("FRED/GDP", start_date="2001-12-31", end_date="2005-12-31", type = "xts") 

# sampling data: 
Quandl("FRED/GDP", collapse="annual")
Quandl("FRED/GDP", transform="rdiff")


mydata = Quandl.datatable("ZACKS/FC",
                          ticker=c("AAPL", "MSFT"),
                          per_end_date.gt="2015-01-01",
                          qopts.columns=c("m_ticker", "per_end_date", "tot_revnu"))


# Tables wth GT 


# Define the start and end dates for the data range

start_date <- "2010-06-07"
end_date <- "2010-06-14"

# Create a gt table based on preprocessed

# `sp500` table data

sp500 %>%
  dplyr::filter(date >= start_date & date <= end_date) %>%
  dplyr::select(-adj_close) %>%
  gt() %>%
  tab_header(
    title = "S&P 500",
    subtitle = glue::glue("{start_date} to {end_date}")
  ) %>%
  
  fmt_date(
    columns = vars(date),
    date_style = 3
  ) %>%
  fmt_currency(
    columns = vars(open, high, low, close),
    currency = "USD"
  ) %>%
  
  fmt_number(
    columns = vars(volume),
    suffixing = TRUE
  )


browseURL("https://rpubs.com/the5ac/all_seasons_portfolio_part1")


# 11) Financial Maths ####
# .......####

# 11.1) Modelling a cash flow ####
#https://bookdown.org/wfoote01/faur/r-warm-ups-in-finance.html

rates <- c(0.06, 0.07, 0.05, 0.09, 0.09, 
           0.08, 0.08, 0.08)
t <- seq(1, 8)

(pv.1 <- sum(1/(1 + rates)^t))

### MQF - MBA Thesis. ---- 

fth  <- read.csv("fth.csv") 
rfth <- fth %>% filter(STATUS %in% c("Approved","Completed","Processing"))
perfomance<-rfth %>% group_by(TYPE) %>% summarise(n=n(),Amount=sum(AMOUNT))
d <-  perfomance %>% slice(1) %>% pull(Amount)
w<-  perfomance %>% slice(2) %>% pull(Amount)
pl <- w-d
trm <- 3250
ppl<-pl*trm


```

